{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Hyperparameter Tuning and Features Seclection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PySparkShell'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check spark app name\n",
    "spark.sparkContext.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.10 (default, Jun  2 2021, 10:49:15) \\n[GCC 9.4.0]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print runtime versions\n",
    "# Python version\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark version\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import databricks.koalas as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ks.read_csv('data/AF_Stock_TW_2603.TW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('key_0', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = ks.DataFrame(df['close'].shift(periods=20).rename('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.set_option('compute.ops_on_diff_frames', True)\n",
    "df = ks.concat([df, tar], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保留前 100 row 空值小於等於 60% 的 column\n",
    "cond = df.iloc[:101,:].isnull().sum()/100 <= 0.6\n",
    "df = df[cond[cond == True].index.to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看 58 row 以後的空值\n",
    "df.iloc[58:,:].isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取 58 row 以後當新的 df # index reset\n",
    "df = df.iloc[58:,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2508, 184)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df.to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "colTrain = list(df.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector assembled set of features \n",
    "# (assemble only the columns you want to MinMaxScale)\n",
    "assembler = VectorAssembler(inputCols=colTrain, outputCol=\"features\")\n",
    "output = assembler.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data for building model\n",
    "model_df = output.select(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data - Train & Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Random Forest to train on the training set\n",
    "train_df, test_df = model_df.randomSplit([0.70, 0.30], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1741, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count(), len(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(767, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count(), len(test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf_model = rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "model_predictions = rf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n",
      "|            features|             label|        prediction|\n",
      "+--------------------+------------------+------------------+\n",
      "|[19.2769391502943...|19.276939392089844| 19.18613926767244|\n",
      "|[19.6624788274294...|18.660076141357425| 19.10773054039324|\n",
      "|[19.8937997357087...|19.276939392089844| 19.18613926767244|\n",
      "|[19.8937998275340...|17.619121551513672|19.019794481807274|\n",
      "|[20.4335541213312...|18.852846145629883| 19.72018766766709|\n",
      "|[21.2046327145946...|19.778139114379883|20.249421125173686|\n",
      "|[21.2817385819022...|19.084169387817383| 19.78803131285374|\n",
      "|[21.5901708137132...| 19.35404586791992|20.057975469364557|\n",
      "|(183,[0,1,2,3,4,5...| 19.97090721130371|20.249421125173686|\n",
      "|[19.4311537671171...|20.317893981933597| 20.30185747590754|\n",
      "+--------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print prediction\n",
    "model_predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatorRMSE = RegressionEvaluator().setLabelCol('label').setPredictionCol(\"prediction\").setMetricName(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.282201095799959"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE = evaluatorRMSE.evaluate(model_predictions)\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatorR2 = RegressionEvaluator().setLabelCol('label').setPredictionCol(\"prediction\").setMetricName(\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9788788645820307"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2 = evaluatorR2.evaluate(model_predictions)\n",
    "R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator()\n",
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [10,20])\n",
    "             .addGrid(rf.maxBins, [20,30])\n",
    "             .addGrid(rf.numTrees, [5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = cv.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best Param(maxDepth): 20'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Best Param(maxDepth): {best_rf_model._java_obj.getMaxDepth()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best Param(maxBins): 30'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Best Param(maxBins): {best_rf_model._java_obj.getMaxBins()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressionModel: uid=RandomForestRegressor_4e4168c47f84, numTrees=5, numFeatures=183"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for entire dataset\n",
    "model_predictions = best_rf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8749532201014778"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE = evaluatorRMSE.evaluate(model_predictions)\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9901649937263322"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2 = evaluatorR2.evaluate(model_predictions)\n",
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(183, {0: 0.0, 1: 0.0001, 2: 0.0, 3: 0.0, 4: 0.0003, 5: 0.0043, 6: 0.0001, 7: 0.0007, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.1355, 12: 0.0, 13: 0.0002, 14: 0.0, 15: 0.0002, 16: 0.1652, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.015, 22: 0.0, 23: 0.0, 24: 0.0011, 25: 0.0, 26: 0.0, 27: 0.0049, 29: 0.0, 30: 0.0003, 33: 0.0003, 35: 0.0142, 36: 0.001, 37: 0.0014, 38: 0.0, 39: 0.0, 40: 0.0006, 41: 0.0, 42: 0.0, 43: 0.0, 44: 0.0001, 45: 0.0001, 46: 0.0001, 47: 0.0, 48: 0.0007, 49: 0.0002, 50: 0.0001, 51: 0.0002, 52: 0.0002, 53: 0.0, 54: 0.0, 55: 0.0002, 56: 0.0001, 57: 0.0, 58: 0.0, 59: 0.0001, 60: 0.0008, 61: 0.0, 62: 0.0, 63: 0.0, 64: 0.0, 65: 0.0013, 66: 0.0001, 67: 0.0002, 68: 0.0, 69: 0.0, 70: 0.0, 71: 0.0007, 72: 0.0001, 73: 0.0, 74: 0.0, 75: 0.0, 76: 0.0, 77: 0.0, 78: 0.0, 79: 0.0003, 80: 0.2938, 81: 0.0, 82: 0.0007, 83: 0.0, 84: 0.0, 85: 0.0001, 86: 0.0003, 87: 0.1565, 88: 0.0, 89: 0.045, 90: 0.0, 93: 0.0, 95: 0.0, 100: 0.0, 102: 0.0, 104: 0.0, 105: 0.0, 106: 0.0, 107: 0.0003, 108: 0.0, 109: 0.0, 112: 0.0, 115: 0.0, 116: 0.0, 117: 0.0, 118: 0.0, 119: 0.0, 121: 0.0, 122: 0.0, 124: 0.0, 128: 0.0, 129: 0.0, 130: 0.0, 131: 0.0, 137: 0.0, 139: 0.0, 140: 0.0, 141: 0.0, 142: 0.0, 146: 0.0, 147: 0.0, 151: 0.0, 152: 0.0002, 153: 0.0, 154: 0.0, 155: 0.0, 156: 0.0014, 157: 0.0003, 158: 0.0, 159: 0.0001, 160: 0.0, 161: 0.0, 162: 0.0, 163: 0.0, 164: 0.0, 165: 0.0138, 166: 0.0047, 167: 0.0, 168: 0.0, 169: 0.0, 170: 0.0, 171: 0.0168, 172: 0.0, 173: 0.0024, 174: 0.0735, 175: 0.0268, 176: 0.0, 177: 0.0001, 178: 0.0001, 179: 0.003, 180: 0.0, 181: 0.0022, 182: 0.0062})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FEATURE IMPORTANCES\n",
    "best_rf_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = ks.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/functions.py:389: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>ma</td>\n",
       "      <td>0.293831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>max_y</td>\n",
       "      <td>0.165241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>sma</td>\n",
       "      <td>0.156475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>max_x</td>\n",
       "      <td>0.135539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>weekofyear</td>\n",
       "      <td>0.073459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>trima</td>\n",
       "      <td>0.044998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>175</td>\n",
       "      <td>dayofyear</td>\n",
       "      <td>0.026751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>171</td>\n",
       "      <td>year</td>\n",
       "      <td>0.016796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>sum</td>\n",
       "      <td>0.014963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>adx</td>\n",
       "      <td>0.014246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx        name     score\n",
       "80    80          ma  0.293831\n",
       "16    16       max_y  0.165241\n",
       "87    87         sma  0.156475\n",
       "11    11       max_x  0.135539\n",
       "174  174  weekofyear  0.073459\n",
       "89    89       trima  0.044998\n",
       "175  175   dayofyear  0.026751\n",
       "171  171        year  0.016796\n",
       "21    21         sum  0.014963\n",
       "35    35         adx  0.014246"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExtractFeatureImp(best_rf_model.featureImportances, model_df, \"features\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection Using Feature Importance Score - Creating a PySpark Estimator\n",
    "# https://www.timlrx.com/blog/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
